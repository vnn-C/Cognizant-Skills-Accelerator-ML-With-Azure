{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lzma\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset = pd.read_csv(\"../data/legal_text_classification.csv\")\\nprint(type(dataset))\\nprint(dataset.columns)\\n\\ndataset_text = dataset[\"case_text\"]\\nprint(dataset_text[0])\\ndataset_text = [f\"summarize: {dtxt}\" for dtxt in dataset_text]\\nprint(dataset_text[0])\\n\\ntrain_text, temp_text = train_test_split(dataset_text, test_size=0.2, random_state=1)\\nvalid_text, test_text = train_test_split(temp_text, test_size=0.5, random_state=1)\\n\\nwith open(\"dataset/legal_train.json\", \"w\") as f:\\n    json.dump({\"input_data\" : train_text}, f)\\nwith open(\"dataset/legal_test.json\", \"w\") as f:\\n    json.dump({\"input_data\" : test_text}, f)\\nwith open(\"dataset/legal_valid.json\", \"w\") as f:\\n    json.dump({\"input_data\" : valid_text}, f)'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data (unused)\n",
    "\"\"\"dataset = pd.read_csv(\"../data/legal_text_classification.csv\")\n",
    "print(type(dataset))\n",
    "print(dataset.columns)\n",
    "\n",
    "dataset_text = dataset[\"case_text\"]\n",
    "print(dataset_text[0])\n",
    "dataset_text = [f\"summarize: {dtxt}\" for dtxt in dataset_text]\n",
    "print(dataset_text[0])\n",
    "\n",
    "train_text, temp_text = train_test_split(dataset_text, test_size=0.2, random_state=1)\n",
    "valid_text, test_text = train_test_split(temp_text, test_size=0.5, random_state=1)\n",
    "\n",
    "with open(\"dataset/legal_train.json\", \"w\") as f:\n",
    "    json.dump({\"input_data\" : train_text}, f)\n",
    "with open(\"dataset/legal_test.json\", \"w\") as f:\n",
    "    json.dump({\"input_data\" : test_text}, f)\n",
    "with open(\"dataset/legal_valid.json\", \"w\") as f:\n",
    "    json.dump({\"input_data\" : valid_text}, f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'try:\\n    with open(\"dataset/compiled_cases.jsonl\", \"w\") as f:\\n        for d, s in zip(json_data[\"Document\"], json_data[\"Summary\"]):\\n            json.dump({\"Document\": d, \"Summary\": s}, f)\\n            f.write(\"\\n\")\\nexcept Exception as e:\\n    print(f\"Error with saving json data: {e}\")'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data for json and convert it into a jsonl file\n",
    "vol_one = \"../data/TexasCaseDataVol1/json\"\n",
    "vol_two = \"../data/TexasCaseDataVol2/json\"\n",
    "vol_three = \"../data/TexasCaseDataVol3/json\"\n",
    "vol_four = \"../data/TexasCaseDataVol4/json\"\n",
    "vol_five = \"../data/TexasCaseDataVol5/json\"\n",
    "#vol_list = [vol_one, vol_two, vol_three, vol_four, vol_five]\n",
    "vol_list = [vol_one, vol_two]\n",
    "json_list = []\n",
    "json_copy = None\n",
    "count = 0\n",
    "try:\n",
    "    for v in vol_list:\n",
    "        for f in os.listdir(v):\n",
    "            path = os.path.join(v, f)\n",
    "            with open(path, 'r') as file:\n",
    "                json_data = {\"article\" : [], \"highlights\" : []}\n",
    "\n",
    "                vol_json = json.load(file)\n",
    "                json_copy = vol_json.copy()\n",
    "                case_body = vol_json[\"casebody\"]\n",
    "                raw_text = case_body[\"opinions\"][0][\"text\"]\n",
    "                summary = case_body[\"head_matter\"]\n",
    "            \n",
    "                \n",
    "                json_data[\"article\"] = \"\".join(raw_text).replace(\"\\n\", \" \")\n",
    "                json_data[\"highlights\"] = \"\".join(summary).replace(\"\\n\", \" \")\n",
    "\n",
    "                json_list.append(json_data)\n",
    "except Exception as e:\n",
    "    print(f\"Error with reading data from files: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"try:\n",
    "    with open(\"dataset/compiled_cases.jsonl\", \"w\") as f:\n",
    "        for d, s in zip(json_data[\"Document\"], json_data[\"Summary\"]):\n",
    "            json.dump({\"Document\": d, \"Summary\": s}, f)\n",
    "            f.write(\"\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with saving json data: {e}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "json_df = pd.DataFrame(json_list)\n",
    "json_df = json_df.dropna()\n",
    "\n",
    "train_df, temp_df = train_test_split(json_df, test_size=0.4, random_state=1)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=1)\n",
    "\n",
    "train_df.to_csv('dataset/train.csv', index=False)\n",
    "test_df.to_csv('dataset/test.csv', index=False)\n",
    "val_df.to_csv('dataset/validate.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
